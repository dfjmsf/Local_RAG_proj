import os
import sys
import httpx

# è¿™è¡Œä»£ç å‘Šè¯‰åº“ï¼šç›´æ¥è¯»æœ¬åœ°ç¼“å­˜ï¼Œç»å¯¹ä¸è¦å°è¯•è¿æ¥ huggingface.co
os.environ["HF_HUB_OFFLINE"] = "1"

os.environ["NO_PROXY"] = "localhost,127.0.0.1"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from openai import OpenAI

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_DIR = os.path.join(CURRENT_DIR, '../data/chroma_db')

class RAGSystem:
    def __init__(self):
        """
                åˆå§‹åŒ– RAG ç³»ç»Ÿï¼šåŠ è½½å‘é‡æ•°æ®åº“å’Œ LLM å®¢æˆ·ç«¯
        """
        print("æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")

        # A. å‡†å¤‡å‘é‡æ¨¡å‹ (å¿…é¡»å’Œ ingest.py ç”¨çš„ä¸€æ¨¡ä¸€æ ·ï¼)
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
        )

        # B. åŠ è½½å·²ç»å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        # çŸ¥è¯†ç‚¹ï¼šæŒä¹…åŒ–åŠ è½½
        # åªè¦æŒ‡å®š persist_directoryï¼ŒChroma å°±ä¼šè‡ªåŠ¨è¯»å–ç¡¬ç›˜ä¸Šçš„æ•°æ®ï¼Œä¸éœ€è¦é‡æ–° ingest
        if not os.path.exists(DB_DIR):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®åº“ç›®å½•: {DB_DIR}ã€‚è¯·å…ˆè¿è¡Œ ingest.pyï¼")

        self.vector_db = Chroma(
            persist_directory=DB_DIR,
            embedding_function=self.embeddings_model
        )

        # C. è¿æ¥æœ¬åœ° LLM
        print(" -> æ­£åœ¨é…ç½®ç½‘ç»œè¿æ¥ (å¼ºåˆ¶ç»•è¿‡ä»£ç†)...")

        # ä½¿ç”¨ trust_env=False
        # è¿™æ„å‘³ç€ï¼šä¸è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼ˆå³å¿½ç•¥ VPN/Proxy è®¾ç½®ï¼‰ï¼Œç›´æ¥ç›´è¿ã€‚
        # è¿™æ˜¯å…¼å®¹æ€§æœ€å¥½çš„â€œç»•è¿‡ä»£ç†â€çš„æ–¹æ³•
        http_client = httpx.Client(
            trust_env=False,
            timeout=120.0  # è®¾ç½®æé•¿çš„è¶…æ—¶æ—¶é—´ (120ç§’)
        )
        self.client = OpenAI(base_url="https://127.0.0.1:1234/v1",
                             api_key="lm-studio",
                             http_client=http_client
                             )
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def query(self, question):
        """
               æ ¸å¿ƒæ–¹æ³•ï¼šæ‰§è¡Œ RAG æµç¨‹
        """
        print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ï¼š{question}")

        # --- æ­¥éª¤ 1: è¯­ä¹‰æ£€ç´¢ (Retrieval) ---
        # k=3 è¡¨ç¤ºæˆ‘ä»¬è¦æ‰¾æœ€ç›¸ä¼¼çš„ 3 ä¸ªç‰‡æ®µ
        # çŸ¥è¯†ç‚¹ï¼šSimilarity Search (ç›¸ä¼¼åº¦æœç´¢)
        # å®ƒä¼šå°† question è½¬ä¸ºå‘é‡ï¼Œè®¡ç®—ä¸æ•°æ®åº“ä¸­æ‰€æœ‰å‘é‡çš„â€œè·ç¦»â€ï¼Œè¿”å›æœ€è¿‘çš„ k ä¸ªã€‚
        docs = self.vector_db.similarity_search(question, k=3)

        if not docs:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
            return

        # æ‰“å°ä¸€ä¸‹æ‰¾åˆ°äº†ä»€ä¹ˆï¼ˆè°ƒè¯•ç”¨ï¼Œè®©ä½ çœ‹åˆ° RAG åˆ°åº•æœåˆ°äº†å•¥,çœŸæ­£ä½¿ç”¨çš„æ—¶å€™æœ€å¥½å…³(æ³¨é‡Š)æ‰ï¼‰
        print("\nğŸ“š æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™ï¼š")
        context_text = ""
        for i, doc in enumerate(docs):
            # source æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œpage æ˜¯é¡µç 
            sourcs = os.path.basename(doc.metadata.get("source", "æœªçŸ¥æ¥æº"))
            page = doc.metadata.get("page", 0) + 1
            content = doc.page_content.replace("\n", " ")   # å»æ‰æ¢è¡Œï¼Œè®©æ˜¾ç¤ºæ›´ç´§å‡‘

            print(f"[{i+1}]...{content[:50]}... (æ¥æº: {sourcs} ç¬¬{page}é¡µ)")

            context_text += f"ç‰‡æ®µ{i+1}: {content}\n"

        # --- æ­¥éª¤ 2: æ„å»º Prompt (Prompt Engineering) ---
        # è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„ RAG æç¤ºè¯æ¨¡ç‰ˆ
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
è¯·å®Œå…¨æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœã€å‚è€ƒèµ„æ–™ã€‘é‡Œæ²¡æœ‰æåˆ°ç­”æ¡ˆï¼Œå°±ç›´æ¥è¯´â€œæˆ‘åœ¨ç°æœ‰æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆâ€ï¼Œä¸è¦ç¼–é€ ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
"""
        user_prompt = f"""
ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{question}
        """

        # --- æ­¥éª¤ 3: è°ƒç”¨ LLM ç”Ÿæˆå›ç­” ---
        print("\n DeepSeek æ­£åœ¨æ€è€ƒ...")
        try:
            response = self.client.chat.completions.create(
                model="local-modal",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1, # RAG ä»»åŠ¡è¦ä½åˆ›é€ æ€§ï¼Œä¿è¯ä¸¥è°¨
                stream=True
            )
            # è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œè®© UI å±‚å»å¤„ç†æµå¼è¾“å‡º
            return response
        except Exception as e:
            # --- è¯¦ç»†çš„é”™è¯¯æ‰“å° ---
            print(f"\nâŒ è°ƒç”¨å¤±è´¥ï¼è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(f"ç±»å‹: {type(e).__name__}")
            print(f"å†…å®¹: {str(e)}")
            return None

if __name__ == "__main__":
    # å®ä¾‹åŒ–ç³»ç»Ÿ
    rag = RAGSystem()

    # è¿™é‡Œçš„æµ‹è¯•é—®é¢˜ï¼Œè¯·æ”¹æˆä½ åˆšæ‰ä¸Šä¼ çš„ PDF é‡ŒåŒ…å«çš„å†…å®¹ï¼
    # æ¯”å¦‚ä½ çš„ PDF æ˜¯è®²â€œåˆåŒæ³•â€çš„ï¼Œä½ å°±é—®â€œè¿çº¦è´£ä»»æ˜¯ä»€ä¹ˆï¼Ÿâ€
    test_question = input("è¯·è¾“å…¥æµ‹è¯•é—®é¢˜:")

    answer_stream = rag.query(test_question)

    if answer_stream:
        print("\nğŸ“¢ å›ç­”ï¼š")
        import time
        for chunk in answer_stream:
            content = chunk.choices[0].delta.content
            if content:
                for char in content:
                    print(char, end="", flush=True)
                    time.sleep(0.02)  #æ ¹æ®ä½ çš„ç”µè„‘æ€§èƒ½è°ƒæ•´å¤§å°,å…¬å¼:1/ä½ æ˜¾å¡(ç”µè„‘)èƒ½è·‘çš„æ¯ç§’tokenæ•°(å‚è€ƒ:NVDIA 5070ti and AMD 9070XTçº¦ä¸º50token/s`; 4060ä»¥ä¸‹å»ºè®®è°ƒç”¨API)
        print("\n")

