import os
import json
import requests  # <--- æ ¸å¿ƒæ”¹å˜ï¼šç”¨æœ€åŸå§‹çš„ requests åº“

# --- 1. å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ ---
os.environ["HF_HUB_OFFLINE"] = "1"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder

# --- 2. è·¯å¾„è®¾ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_DIR = os.path.join(CURRENT_DIR, "../data/chroma_db")
RERANK_MODEL_PATH = os.path.join(CURRENT_DIR, "../model_cache/bge-reranker-base")


class RAGSystem:
    def __init__(self):
        print("æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")

        # A. å‘é‡æ¨¡å‹
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"}
        )

        # B. å‘é‡æ•°æ®åº“
        if not os.path.exists(DB_DIR):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®åº“ç›®å½•: {DB_DIR}")

        self.vector_db = Chroma(
            persist_directory=DB_DIR,
            embedding_function=self.embedding_model
        )

        # C. åˆå§‹åŒ– Rerank æ¨¡å‹
        print(f" -> æ­£åœ¨åŠ è½½ Rerank æ¨¡å‹ ({RERANK_MODEL_PATH})...")
        try:
            # device="cpu" ä¿è¯å…¼å®¹æ€§ï¼Œæœ‰ N å¡å¯ä»¥æ”¹æˆ "cuda"
            self.reranker = CrossEncoder(RERANK_MODEL_PATH, device="cpu")
            print(" -> Rerank æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ Rerank æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   (å°†è‡ªåŠ¨é™çº§ä¸ºä»…ä½¿ç”¨å‘é‡æ£€ç´¢)")
            self.reranker = None

        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def route_query(self, question):
        """
        åˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼šæ˜¯éœ€è¦æ£€ç´¢(SEARCH)è¿˜æ˜¯é—²èŠ(CHAT)
        """
        print(f"ğŸš¦ æ­£åœ¨è¿›è¡Œæ„å›¾è·¯ç”±åˆ†æ: {question}")

        # æç®€ Promptï¼Œå¼ºåˆ¶æ¨¡å‹åªè¾“å‡ºå…³é”®è¯
        # æç®€ Promptï¼Œå¼ºåˆ¶æ¨¡å‹åªè¾“å‡ºå…³é”®è¯
        system_prompt = (
            "You are a routing system. Analyze the user's question. "
            "If the question implies looking up specific documents, facts, or context, output 'SEARCH'. "
            "If the question is a greeting, general knowledge, coding request, or translation, output 'CHAT'. "
            "Output ONLY 'SEARCH' or 'CHAT'. Do not explain."
        )

        try:
            url = "http://127.0.0.1:1234/v1/chat/completions"
            headers = {"Content-Type": "application/json"}
            data = {
                "model" : "local-model",
                "messages" : [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                "temperature" : 0.0,
                "max_tokens" : 1000,
                "stream" : False
            }

            response = requests.post(
                url, headers = headers , json = data,
                proxies = {"http" : None, "https" : None}, timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                raw_content = result['choices'][0]['message']['content'].strip()

                # [æ–°å¢] è°ƒè¯•æ‰“å°ï¼šçœ‹çœ‹æ¨¡å‹åˆ°åº•è¾“å‡ºäº†ä»€ä¹ˆå¦–é­”é¬¼æ€ª
                print(f"   [Debug] è·¯ç”±åŸå§‹è¾“å‡º: {raw_content}")

                # [ä¿®æ”¹ 2] æ¸…æ´— <think> æ ‡ç­¾
                # DeepSeek-R1 å–œæ¬¢è¾“å‡º <think>æ€è€ƒè¿‡ç¨‹</think> SEARCH
                final_intent = raw_content
                if "</think>" in raw_content:
                    # åªå– </think> åé¢çš„éƒ¨åˆ†
                    final_intent = raw_content.split("</think>")[-1].strip()

                final_intent = final_intent.upper()

                # [ä¿®æ”¹ 3] åˆ¤å®šé€»è¾‘
                # åªè¦åŒ…å« CHAT å°±è®¤ä¸ºæ˜¯é—²èŠï¼Œå¦åˆ™é»˜è®¤ SEARCH (æ›´å®‰å…¨çš„ç­–ç•¥)
                if "CHAT" in final_intent:
                    return "CHAT"

                return "SEARCH"

            print(f"âŒ è·¯ç”±APIæŠ¥é”™: {response.status_code}")
            return "SEARCH"  # å¤±è´¥é»˜è®¤èµ°æœç´¢

        except Exception as e:
            print(f"âŒ è·¯ç”±å¤±è´¥: {e}ï¼Œé»˜è®¤èµ° SEARCH")
            return "SEARCH"

    #  æŠ½ç¦»å‡ºçš„ LLM è°ƒç”¨é€šç”¨å‡½æ•°
    def _call_llm(self, messages):
        print("\nğŸ¤– DeepSeek æ­£åœ¨æ€è€ƒ...")
        url = "http://127.0.0.1:1234/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "local-model",
            "messages": messages,
            "temperature": 0.3,
            "stream": True
        }
        try:
            response = requests.post(
                url, headers=headers, json=data, stream=True,
                proxies={"http": None, "https": None}, timeout=60
            )
            if response.status_code != 200:
                print(f"âŒ æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
                print(response.text)
                return None
            return response
        except Exception as e:
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            return None


    def query(self, question, history=[], mode="flash", use_reranker=False):
        """
        :param question: ç”¨æˆ·é—®é¢˜
        :param mode: 'flash' (æé€Ÿ) æˆ– 'pro' (æ·±åº¦)
        :param history: å‰ç«¯ä¼ æ¥çš„å†å²å¯¹è¯åˆ—è¡¨ (list of dict)
        :return: (responseå¯¹è±¡, å‚è€ƒæ–‡æ¡£åˆ—è¡¨)
        """

        # 1. [æ–°å¢] æ‰§è¡Œè·¯ç”±åˆ¤æ–­
        intent = self.route_query(question)
        print(f"ğŸ‘‰ è·¯ç”±ç»“æœ: {intent}")

        print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ï¼š{question} | æ¨¡å¼: {mode.upper()}")

        final_docs = []
        search_query = question

        # === åˆ†æ”¯ A: é—²èŠæ¨¡å¼ (CHAT) ===
        if intent == "CHAT":
            # ç›´æ¥æ„å»º Promptï¼Œä¸æŸ¥åº“
            print("ğŸ’¬ è¿›å…¥é—²èŠæ¨¡å¼ï¼Œè·³è¿‡æ£€ç´¢...")

            # ä½¿ç”¨æ›´é€šç”¨çš„ System Prompt
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            messages_payload = [{"role": "system", "content": system_prompt}]
            # æ³¨å…¥å†å²
            if history:
                messages_payload.extend(history[-6:])
            messages_payload.append({"role": "user", "content": question})

            # ç›´æ¥è°ƒç”¨ LLM
            response = self._call_llm(messages_payload)
            # è¿”å›æ—¶ doc åˆ—è¡¨ä¸ºç©ºï¼Œå‰ç«¯å°±ä¸ä¼šæ˜¾ç¤ºâ€œå‚è€ƒæ¥æºâ€
            return response, [], intent  # æŠŠ intent ä¹Ÿè¿”å›ç»™å‰ç«¯ç”¨äºå±•ç¤º

        # === åˆ†æ”¯ B: æ£€ç´¢æ¨¡å¼ (SEARCH) ===
        else: # intent == "SEARCH"
            print("ğŸ” è¿›å…¥æ£€ç´¢æ¨¡å¼...")

            # --- æ­¥éª¤ 1: æ£€ç´¢ç­–ç•¥åˆ†æµ ---
            if mode == "pro" and self.reranker:
                # === Pro æ¨¡å¼ (æ·±åº¦) ===
                # 1. æ‰©å¤§å¬å›ï¼šå…ˆæå‡º 20 æ¡ (Top-20)
                initial_docs = self.vector_db.similarity_search(question, k=20)

                if initial_docs:
                    # 2. å‡†å¤‡é…å¯¹æ•°æ® [é—®é¢˜, æ–‡æ¡£å†…å®¹]
                    pairs = [[question, doc.page_content] for doc in initial_docs]

                    # 3.æ¨¡å‹æ‰“åˆ†
                    print(" -> æ­£åœ¨è¿›è¡Œ Rerank é‡æ’åº...")
                    scores = self.reranker.predict(pairs)

                    # 4. æ’åºæˆªæ–­ (Top-5)
                    # å°†æ–‡æ¡£å’Œåˆ†æ•°æ‰“åŒ…ï¼ŒæŒ‰åˆ†æ•°é™åºæ’
                    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)

                    print("\nğŸ“Š Rerank æ‰“åˆ†ç»“æœ (Top-5):")
                    for doc, score in scored_docs[:5]:
                        print(f"   [åˆ†: {score:.4f}] {doc.page_content[:30]}...")

                    # å–å‰ 5 åçš„æ–‡æ¡£å¯¹è±¡
                    final_docs = [doc for doc, score in scored_docs[:5]]
                else:
                    print("âš ï¸ åˆæ­¥æ£€ç´¢æœªæ‰¾åˆ°æ–‡æ¡£ã€‚")

            else:
                # === Flash æ¨¡å¼ (æé€Ÿ) ===
                # ç›´æ¥æ‰¾ Top-5ï¼Œä¸ç»è¿‡æ¨¡å‹é‡ç®—ï¼Œé€Ÿåº¦æœ€å¿«
                final_docs = self.vector_db.similarity_search(question, k= 5)


            # --- é€šç”¨é€»è¾‘ ---
            if not final_docs:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                return None, []

            print("\nğŸ“š æœ€ç»ˆå‚è€ƒèµ„æ–™ (Parent-Child è¿˜åŸ)ï¼š")
            context_text = ""
            used_parents = set() # ç”¨äºå»é‡ï¼Œé˜²æ­¢å¤šä¸ªå­å—å±äºåŒä¸€ä¸ªçˆ¶å—ï¼Œå¯¼è‡´é‡å¤é˜…è¯»

            for i, doc in enumerate(final_docs):
                # [å…³é”®] ä¼˜å…ˆå°è¯•ä» metadata è·å–çˆ¶æ–‡æ¡£å†…å®¹
                # å¦‚æœæ˜¯æ—§æ•°æ®åº“æ²¡æœ‰ parent_contentï¼Œåˆ™å›é€€ä½¿ç”¨ doc.page_content
                content = doc.metadata.get("parent_content", doc.page_content)

                # [å»é‡é€»è¾‘]
                # è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼æˆ–ç›´æ¥ç”¨å­—ç¬¦ä¸²åˆ¤æ–­ï¼Œé˜²æ­¢é‡å¤æ·»åŠ ç›¸åŒçš„çˆ¶æ–‡æ¡£
                # è¿™é‡Œç®€å•ç”¨å­—ç¬¦ä¸²é•¿åº¦+å‰100å­—ä½œä¸ºç®€æ˜“æŒ‡çº¹
                content_fingerprint = f"{len(content)}_{content[:50]}"

                if content_fingerprint in used_parents:
                    print(f"   [è·³è¿‡] å­å— {i+1} æŒ‡å‘å·²å­˜åœ¨çš„çˆ¶å—...")
                    continue

                used_parents.add(content_fingerprint)

                # æ‰“å°é¢„è§ˆ (é¢„è§ˆä¸€ä¸‹å­å—çš„æ¥æº)
                source = os.path.basename(doc.metadata.get("source", "unknown"))
                cleaned_content = content[:50].replace('\n', '')
                print(f"[{len(used_parents)}] æ¥æº: {source} | å†…å®¹é¢„è§ˆ: {cleaned_content}...")
                # æ‹¼æ¥åˆ° Context
                context_text += f"ç‰‡æ®µ{len(used_parents)}: {content}\n\n"

        # --- æ­¥éª¤ 2: æ„å»º Prompt ä¸ å†å²æ¶ˆæ¯æ³¨å…¥ ---
        # 1. å®šä¹‰ç³»ç»Ÿæç¤ºè¯ (Persona)
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“ã€‚"

        # 2. åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
        messages_payload = [
            {"role": "system", "content": system_prompt}
        ]

        # 3. æ³¨å…¥å†å²è®°å¿† (Sliding Window)
        # åªä¿ç•™æœ€è¿‘çš„ 6 æ¡æ¶ˆæ¯ (å³ 3 è½®å¯¹è¯)ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡è¶…é™
        # history æ ¼å¼: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
        if history:
            recent_history = history[-6:]
            for msg in recent_history:
                messages_payload.append(msg)
            print(f" -> å·²æ³¨å…¥å†å²è®°å¿†: {len(recent_history)} æ¡æ¶ˆæ¯")

        # 4. æ‹¼æ¥å½“å‰æœ€æ–°çš„ User Prompt (åŒ…å« RAG ä¸Šä¸‹æ–‡)
        current_user_prompt = f"ã€å‚è€ƒèµ„æ–™ã€‘:\n{context_text}\n\nã€é—®é¢˜ã€‘:\n{question}"
        messages_payload.append({"role": "user", "content": current_user_prompt})

        # --- æ­¥éª¤ 3: è°ƒç”¨ LLM (ä½¿ç”¨ requests æš´åŠ›ç›´è¿) ---
        response = self._call_llm(messages_payload)

        # è¿”å› 3 ä¸ªå€¼: å“åº”æµ, æ–‡æ¡£åˆ—è¡¨, æ„å›¾
        return response, final_docs, intent

if __name__ == "__main__":
    rag = RAGSystem()

    # è·å– response å¯¹è±¡
    test_question = input("è¯·è¾“å…¥æµ‹è¯•é—®é¢˜:")
    response = rag.query(test_question, mode="pro")  # é»˜è®¤æµ‹è¯• Pro æ¨¡å¼



    if response:
        print("\nğŸ“¢ å›ç­”ï¼š")
        import time

        # æ‰‹åŠ¨è§£ææµå¼æ•°æ® (Parsing SSE)
        for line in response.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                # è¿‡æ»¤æ‰ "data: " å‰ç¼€
                if decoded_line.startswith("data: "):
                    json_str = decoded_line[6:]  # å»æ‰å‰6ä¸ªå­—ç¬¦
                    if json_str.strip() == "[DONE]":
                        break
                    try:
                        json_data = json.loads(json_str)
                        content = json_data['choices'][0]['delta'].get('content', '')
                        if content:
                            for char in content:
                                print(char, end="", flush=True)
                                time.sleep(0.01)
                    except json.JSONDecodeError:
                        continue
        print("\n")