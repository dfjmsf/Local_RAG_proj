import os
import json
import requests  # <--- æ ¸å¿ƒæ”¹å˜ï¼šç”¨æœ€åŸå§‹çš„ requests åº“

# --- 1. å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ ---
os.environ["HF_HUB_OFFLINE"] = "1"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder

# --- 2. è·¯å¾„è®¾ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_DIR = os.path.join(CURRENT_DIR, "../data/chroma_db")
RERANK_MODEL_PATH = os.path.join(CURRENT_DIR, "../model_cache/bge-reranker-base")


class RAGSystem:
    def __init__(self):
        print("æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")

        # A. å‘é‡æ¨¡å‹
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"}
        )

        # B. å‘é‡æ•°æ®åº“
        if not os.path.exists(DB_DIR):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®åº“ç›®å½•: {DB_DIR}")

        self.vector_db = Chroma(
            persist_directory=DB_DIR,
            embedding_function=self.embedding_model
        )

        # C. åˆå§‹åŒ– Rerank æ¨¡å‹
        print(f" -> æ­£åœ¨åŠ è½½ Rerank æ¨¡å‹ ({RERANK_MODEL_PATH})...")
        try:
            # device="cpu" ä¿è¯å…¼å®¹æ€§ï¼Œæœ‰ N å¡å¯ä»¥æ”¹æˆ "cuda"
            self.reranker = CrossEncoder(RERANK_MODEL_PATH, device="cpu")
            print(" -> Rerank æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ Rerank æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   (å°†è‡ªåŠ¨é™çº§ä¸ºä»…ä½¿ç”¨å‘é‡æ£€ç´¢)")
            self.reranker = None

        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def query(self, question, history=[], mode="flash"):
        """
        :param question: ç”¨æˆ·é—®é¢˜
        :param mode: 'flash' (æé€Ÿ) æˆ– 'pro' (æ·±åº¦)
        :param history: å‰ç«¯ä¼ æ¥çš„å†å²å¯¹è¯åˆ—è¡¨ (list of dict)
        :return: (responseå¯¹è±¡, å‚è€ƒæ–‡æ¡£åˆ—è¡¨)
        """
        print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ï¼š{question} | æ¨¡å¼: {mode.upper()}")

        final_docs = []

        # --- æ­¥éª¤ 1: æ£€ç´¢ç­–ç•¥åˆ†æµ ---
        if mode == "pro" and self.reranker:
            # === Pro æ¨¡å¼ (æ·±åº¦) ===
            # 1. æ‰©å¤§å¬å›ï¼šå…ˆæå‡º 20 æ¡ (Top-20)
            initial_docs = self.vector_db.similarity_search(question, k=20)

            if initial_docs:
                # 2. å‡†å¤‡é…å¯¹æ•°æ® [é—®é¢˜, æ–‡æ¡£å†…å®¹]
                pairs = [[question, doc.page_content] for doc in initial_docs]

                # 3.æ¨¡å‹æ‰“åˆ†
                print(" -> æ­£åœ¨è¿›è¡Œ Rerank é‡æ’åº...")
                scores = self.reranker.predict(pairs)

                # 4. æ’åºæˆªæ–­ (Top-3)
                # å°†æ–‡æ¡£å’Œåˆ†æ•°æ‰“åŒ…ï¼ŒæŒ‰åˆ†æ•°é™åºæ’
                scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)

                print("\nğŸ“Š Rerank æ‰“åˆ†ç»“æœ (Top-5):")
                for doc, score in scored_docs[:5]:
                    print(f"   [åˆ†: {score:.4f}] {doc.page_content[:30]}...")

                # å–å‰ 3 åçš„æ–‡æ¡£å¯¹è±¡
                final_docs = [doc for doc, score in scored_docs[:5]]
            else:
                print("âš ï¸ åˆæ­¥æ£€ç´¢æœªæ‰¾åˆ°æ–‡æ¡£ã€‚")

        else:
            # === Flash æ¨¡å¼ (æé€Ÿ) ===
            # ç›´æ¥æ‰¾ Top-3ï¼Œä¸ç»è¿‡æ¨¡å‹é‡ç®—ï¼Œé€Ÿåº¦æœ€å¿«
            final_docs = self.vector_db.similarity_search(question, k= 5)


        # --- é€šç”¨é€»è¾‘ ---
        if not final_docs:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
            return None, []

        print("\nğŸ“š æœ€ç»ˆå‚è€ƒèµ„æ–™ï¼š")
        context_text = ""
        for i, doc in enumerate(final_docs):
            content = doc.page_content.replace("\n", " ")
            print(f"[{i + 1}] {content[:50]}...")
            # é™åˆ¶é•¿åº¦é˜²æ­¢çˆ†æ˜¾å­˜
            context_text += f"ç‰‡æ®µ{i + 1}: {content[:500]}\n"

        # --- æ­¥éª¤ 2: æ„å»º Prompt ä¸ å†å²æ¶ˆæ¯æ³¨å…¥ ---
        # 1. å®šä¹‰ç³»ç»Ÿæç¤ºè¯ (Persona)
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“ã€‚åœ¨å›ç­”ä¹‹å‰è¯·é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜ä¸è¦æ±‚å¯¹ç”¨æˆ·è¿›è¡Œç®€çŸ­çš„å¤¸å¥–"

        # 2. åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
        messages_payload = [
            {"role": "system", "content": system_prompt}
        ]

        # 3. æ³¨å…¥å†å²è®°å¿† (Sliding Window)
        # åªä¿ç•™æœ€è¿‘çš„ 6 æ¡æ¶ˆæ¯ (å³ 3 è½®å¯¹è¯)ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡è¶…é™
        # history æ ¼å¼: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
        if history:
            recent_history = history[-6:]
            for msg in recent_history:
                messages_payload.append(msg)
            print(f" -> å·²æ³¨å…¥å†å²è®°å¿†: {len(recent_history)} æ¡æ¶ˆæ¯")

        # 4. æ‹¼æ¥å½“å‰æœ€æ–°çš„ User Prompt (åŒ…å« RAG ä¸Šä¸‹æ–‡)
        current_user_prompt = f"ã€å‚è€ƒèµ„æ–™ã€‘:\n{context_text}\n\nã€é—®é¢˜ã€‘:\n{question}"
        messages_payload.append({"role": "user", "content": current_user_prompt})

        # --- æ­¥éª¤ 3: è°ƒç”¨ LLM (ä½¿ç”¨ requests æš´åŠ›ç›´è¿) ---
        print("\nğŸ¤– DeepSeek æ­£åœ¨æ€è€ƒ...")

        url = "http://127.0.0.1:1234/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "local-model",
            "messages": messages_payload,
            "temperature": 0.7,
            "stream": True  # å¼€å¯æµå¼è¾“å‡º
        }

        try:
            # proxies={"http": None, "https": None} æ˜¯æ ¸æ­¦å™¨
            # å®ƒå¼ºåˆ¶ requests åº“å®Œå…¨å¿½ç•¥ç³»ç»Ÿçš„ä»»ä½•ä»£ç†è®¾ç½®
            response = requests.post(
                url,
                headers=headers,
                json=data,
                stream=True,
                proxies={"http": None, "https": None},
                timeout=60
            )

            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code != 200:
                print(f"âŒ æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
                print(response.text)
                return None, []

            return response, final_docs

        except Exception as e:
            print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
            return None, []


if __name__ == "__main__":
    rag = RAGSystem()

    # è·å– response å¯¹è±¡
    test_question = input("è¯·è¾“å…¥æµ‹è¯•é—®é¢˜:")
    response = rag.query(test_question, mode="pro")  # é»˜è®¤æµ‹è¯• Pro æ¨¡å¼



    if response:
        print("\nğŸ“¢ å›ç­”ï¼š")
        import time

        # æ‰‹åŠ¨è§£ææµå¼æ•°æ® (Parsing SSE)
        for line in response.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                # è¿‡æ»¤æ‰ "data: " å‰ç¼€
                if decoded_line.startswith("data: "):
                    json_str = decoded_line[6:]  # å»æ‰å‰6ä¸ªå­—ç¬¦
                    if json_str.strip() == "[DONE]":
                        break
                    try:
                        json_data = json.loads(json_str)
                        content = json_data['choices'][0]['delta'].get('content', '')
                        if content:
                            for char in content:
                                print(char, end="", flush=True)
                                time.sleep(0.01)
                    except json.JSONDecodeError:
                        continue
        print("\n")